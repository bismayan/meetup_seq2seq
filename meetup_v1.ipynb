{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a simple calculator using Sequence to Sequence Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we shall design a simple calculator using a sequence to sequence network. Now ofcourse this example is not supposed to be of any practical use, but the exact same network can be used to create a translator or a summarizer or any other sequence to sequence solution by just changing the input and output training sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some of the basic libraries required to create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge here is to design a calculator which can do the arithmetic operations of addition and subtraction (You can include other operations if you want) on two numbers. However we shall feed the entire input expression as a string and expect the output as a string too. The two numbers themselves can range from -9999 to 9999. The advantage of choosing this problem is that we can dynamically create as large a dataset as we want. Here we start off with dataset of 1 Million examples (Descrease this number of your training is going too slowly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_ops=[\"+\",\"-\"]\n",
    "min_input=-9999\n",
    "max_input=9999\n",
    "dataset_size=1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to generate the dataset. We take any two random integers between max_input and min_input and put a random arithmetic operator from the allowed operator list between them. We then evaluate the resulting expression to get our output. Finally we return both the input expression and the output as a tuple of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(dataset_size):\n",
    "    \"\"\"Generates pairs of equations and solutions to them.\n",
    "    \n",
    "       Each equation has a form of two integers with an operator in between.\n",
    "       Each solution is an integer with the result of the operation.\n",
    "        dataset_size: an integer, number of equations to be generated.\n",
    "        result: a list of tuples of strings (equation, solution).\n",
    "    \"\"\"\n",
    "    sample=[]\n",
    "    random.seed(42)\n",
    "    for i in range(dataset_size):\n",
    "        num1= random.randint(min_input, max_input)\n",
    "        num2= random.randint(min_input, max_input)\n",
    "        op= random.choice(allowed_ops)\n",
    "        eqn= str(num1)+str(op)+str(num2)\n",
    "        result= eval(eqn)\n",
    "        sample.append((eqn, str(result)))\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the function to get our dataset. We then do a standard train_test split (80-20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= generate_data(dataset_size)\n",
    "train_data,test_data= train_test_split(data, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the data we just generated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 training Samples [('439-6908', '-6469'), ('1145+-6208', '-5063'), ('-5787-6002', '-11789')]\n",
      "Train_set_size= 800000\n",
      "First 3 test Samples [('-7973--3050', '-4923'), ('-7352--1411', '-5941'), ('-3397+8826', '5429')]\n",
      "Test_set_size= 200000\n"
     ]
    }
   ],
   "source": [
    "print(\"First 3 training Samples\",train_data[0:3])\n",
    "print(\"Train_set_size=\",len(train_data))\n",
    "print(\"First 3 test Samples\",test_data[0:3])\n",
    "print(\"Test_set_size=\",len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now define three additional special tokens to denote the start of the sequence (required for the decoder), the end of the sequence and a padding character which we shall use to bring our sequences to the same length while passing to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol=\"^\"\n",
    "end_symbol=\"$\"\n",
    "padding_symbol=\"#\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append the end symbol at the end of our input and output data and the start symbol and the end of our output data. Note that we denote inputs as X and outputs as Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=[i[0]+end_symbol for i in train_data]\n",
    "train_Y=[start_symbol+i[1]+end_symbol for i in train_data]\n",
    "test_X=[i[0]+end_symbol for i in test_data]\n",
    "test_Y=[start_symbol+i[1]+end_symbol for i in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 training Samples ['439-6908$', '1145+-6208$', '-5787-6002$']\n",
      "First 3 training output ['^-6469$', '^-5063$', '^-11789$']\n",
      "First 3 test Samples ['-7973--3050$', '-7352--1411$', '-3397+8826$']\n",
      "First 3 test output ['^-4923$', '^-5941$', '^5429$']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 3 training Samples\",train_X[0:3])\n",
    "print(\"First 3 training output\",train_Y[0:3])\n",
    "print(\"First 3 test Samples\",test_X[0:3])\n",
    "print(\"First 3 test output\",test_Y[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is tokenizing the data. Keras provides some handy tools for this purpose. We shall import the Tokenizer and pad_sequences modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit two different tokenizers for the X and Y sequences. Remember to only fit them on the training data. In this particular problem we shall be encoding characters and not words, so we set char_level=True. By default Keras filters out any punctuations and special characters. We don't want this so we set filters to None. \n",
    "\n",
    "Note- Although we dont encounter this in our problem, in general you will have some characters in our training set that are not present in the test set. In order to deal with this, the keras tokenizer provides the oov_token argument. All such unseen strings will be given the oov_token id if it is present. If you leave it as unspecified, all unseen words/ characters will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenizer= Tokenizer(filters=None, char_level=True)\n",
    "X_tokenizer.fit_on_texts(train_X)\n",
    "Y_tokenizer= Tokenizer(filters=None, char_level=True)\n",
    "Y_tokenizer.fit_on_texts(train_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the tokenized versions of the input and output strings using the tokenizers. We also create a dictionary storing the mappings from character to id and vice versa. Note that we also add an token entry of 0 for the padding symbol. This is because the padding we shall do later by default pads the sequences with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tokenized= X_tokenizer.texts_to_sequences(train_X)\n",
    "train_Y_tokenized= Y_tokenizer.texts_to_sequences(train_Y)\n",
    "test_X_tokenized= X_tokenizer.texts_to_sequences(test_X)\n",
    "test_Y_tokenized= Y_tokenizer.texts_to_sequences(test_Y)\n",
    "input_char2id={**X_tokenizer.word_index,**{padding_symbol:0}}\n",
    "input_id2char={j:i for (i,j) in input_char2id.items()}\n",
    "output_char2id={**Y_tokenizer.word_index,**{padding_symbol:0}}\n",
    "output_id2char={j:i for (i,j) in output_char2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pad all our input and output sequences to the same length before we pass into the network. We define the maximum length of the input and output sequences and pad all sequences to this length. The pad_sequences functions allows us to pad/ truncate all sequences to a specified length. We also specify that we want the padding/truncating to be done after the end of the sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_max_len=13\n",
    "output_max_len=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_padded=pad_sequences(train_X_tokenized,maxlen=input_max_len,padding=\"post\",truncating=\"post\",value=input_char2id[padding_symbol])\n",
    "test_X_padded=pad_sequences(test_X_tokenized,maxlen=input_max_len,padding=\"post\",truncating=\"post\",value=input_char2id[padding_symbol])\n",
    "train_Y_padded=pad_sequences(train_Y_tokenized,maxlen=output_max_len,padding=\"post\",truncating=\"post\",value=output_char2id[padding_symbol])\n",
    "test_Y_padded=pad_sequences(test_Y_tokenized,maxlen=output_max_len,padding=\"post\",truncating=\"post\",value=output_char2id[padding_symbol])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the results of the data preparation we just did. First lets have a look at the input token to character mappings and the reverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to ID mapping for Input Sequences: {'-': 1, '$': 2, '5': 3, '1': 4, '7': 5, '4': 6, '9': 7, '2': 8, '6': 9, '8': 10, '3': 11, '0': 12, '+': 13, '#': 0}\n",
      "ID to Character mapping for Input Sequences: {1: '-', 2: '$', 3: '5', 4: '1', 5: '7', 6: '4', 7: '9', 8: '2', 9: '6', 10: '8', 11: '3', 12: '0', 13: '+', 0: '#'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Character to ID mapping for Input Sequences:\",input_char2id)\n",
    "print(\"ID to Character mapping for Input Sequences:\",input_id2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same for the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to ID mapping for Output Sequences: {'^': 1, '$': 2, '1': 3, '-': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, '0': 13, '#': 0}\n",
      "ID to Character mapping for Output Sequences: {1: '^', 2: '$', 3: '1', 4: '-', 5: '2', 6: '3', 7: '4', 8: '5', 9: '6', 10: '7', 11: '8', 12: '9', 13: '0', 0: '#'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Character to ID mapping for Output Sequences:\",output_char2id)\n",
    "print(\"ID to Character mapping for Output Sequences:\",output_id2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at the actual sequences themselves, we define a small utility function that maps a sequence of tokens to characters. We shall use it to check that everything is workinf correctly here. We shall also use it later while displaying outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chars(seq,id2char):\n",
    "    return [id2char[i] for i in seq]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if everything made sense for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Input string: 439-6908$\n",
      "Tokenized Input: [6, 11, 7, 1, 9, 7, 12, 10, 2]\n",
      "Padded Input: [ 6 11  7  1  9  7 12 10  2  0  0  0  0]\n",
      "Tokenized Input converted back to characters: ['4', '3', '9', '-', '6', '9', '0', '8', '$', '#', '#', '#', '#']\n",
      "Raw Output string: ^-6469$\n",
      "Tokenized Output: [1, 4, 9, 7, 9, 12, 2]\n",
      "Padded Output: [ 1  4  9  7  9 12  2]\n",
      "Tokenized Output converted back to characters: ['^', '-', '6', '4', '6', '9', '$']\n"
     ]
    }
   ],
   "source": [
    "id=0\n",
    "print(\"Raw Input string:\",train_X[id])\n",
    "print(\"Tokenized Input:\",train_X_tokenized[id])\n",
    "print(\"Padded Input:\",train_X_padded[id])\n",
    "print(\"Tokenized Input converted back to characters:\",convert_to_chars(train_X_padded[id],input_id2char))\n",
    "print(\"Raw Output string:\",train_Y[id])\n",
    "print(\"Tokenized Output:\",train_Y_tokenized[id])\n",
    "print(\"Padded Output:\",train_Y_padded[id])\n",
    "print(\"Tokenized Output converted back to characters:\",convert_to_chars(train_Y_padded[id],output_id2char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Input string: -7973--3050$\n",
      "Tokenized Input: [1, 5, 7, 5, 11, 1, 1, 11, 12, 3, 12, 2]\n",
      "Padded Input: [ 1  5  7  5 11  1  1 11 12  3 12  2  0]\n",
      "Tokenized Input converted back to characters: ['-', '7', '9', '7', '3', '-', '-', '3', '0', '5', '0', '$', '#']\n",
      "Raw Output string: ^-4923$\n",
      "Tokenized Output: [1, 4, 7, 12, 5, 6, 2]\n",
      "Padded Output: [ 1  4  7 12  5  6  2]\n",
      "Tokenized Output converted back to characters: ['^', '-', '4', '9', '2', '3', '$']\n"
     ]
    }
   ],
   "source": [
    "id=0\n",
    "print(\"Raw Input string:\",test_X[id])\n",
    "print(\"Tokenized Input:\",test_X_tokenized[id])\n",
    "print(\"Padded Input:\",test_X_padded[id])\n",
    "print(\"Tokenized Input converted back to characters:\",convert_to_chars(test_X_padded[id],input_id2char))\n",
    "print(\"Raw Output string:\",test_Y[id])\n",
    "print(\"Tokenized Output:\",test_Y_tokenized[id])\n",
    "print(\"Padded Output:\",test_Y_padded[id])\n",
    "print(\"Tokenized Output converted back to characters:\",convert_to_chars(test_Y_padded[id],output_id2char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definining Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the size of the input and output vocabularies, the size of the input and output embedding vectors for the characters and the output size for the Encoder and decoder RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size=len(input_char2id.keys())\n",
    "input_embedding_size=20\n",
    "output_vocab_size=len(output_char2id.keys())\n",
    "output_embedding_size=20\n",
    "hidden_size=512 ## Output size for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required modules for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Encoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder model itself is very simple (Just 4 lines of code). It starts off with an Input layer. The Inputs are then fed through an Embedding layers. The embedded inputs then pass through a batch normalization layer. Batch Normalization normalizes the output between batches and helps with training stability. It is not really required for this problem but the idea was to be able to use this network for much more complicated problems. After Batch Normalization, we pass the inputs to a Bidirectional GRU. A bidirectional RNN is a combination of 2 RNNs , one which reads the forward sequence and other reads the reverse sequence. The outputs for the two RNN's are then averaged to get the final GRU output. This is the output we shall pass to the Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K.clear_session()\n",
    "tf.reset_default_graph() ## These two commands clear any old graphs you might have in memory. Useful for retraining\n",
    "\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(input_max_len,), name='Encoder-Input')\n",
    "x = Embedding(input_vocab_size,input_embedding_size, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "out_gru= Bidirectional(GRU(hidden_size),merge_mode=\"ave\")(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder Model is very similar in nature too. Note that while training we shall be using teacher forcing, so we feed in the actual output sequences (token by token until the penultimate token) to the RNN. For each element of the output sequence that we feed in, we then require the Decoder to output the next element of the output sequence correctly. So we feed in these teacher forcing inputs via the input layer, which then pass through and embedding and a batch_normalization layer. We then feed  these sequences to the GRU decoder RNN. Note that to get all the outputs for an RNN intead of just the final output, we have to specify return_sequences=True in the GRU. We also feed the output of the encoder to the decoder RNN using the initial state parameter. The outputs of the RNN are then put through a single dense (fully connected) layer with output size= output vocabulary size with a softmax non-linearity. \n",
    "\n",
    "At the end of the day we are solving a multiclass classification problem for each character, so the output is a probability distribution for the output character at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "dec_emb = Embedding(output_vocab_size,output_embedding_size, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(hidden_size,return_sequences=True,name='Decoder-GRU')\n",
    "decoder_gru_output= decoder_gru(dec_bn,initial_state=out_gru)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the two into a Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall model is just a combination of the two. We simply define the input and output layers we are using to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compile the model. As noted earlier, we are doing a multiclass classification, so the loss is categorical crossentropy. We use the sparse version to avoid going through the hassle of one hot encoding the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model.compile(optimizer=Adam(lr=0.01), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 538.50 483.00\" width=\"539pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-479 534.5,-479 534.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140124662069736 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140124662069736</title>\n",
       "<polygon fill=\"none\" points=\"47,-438.5 47,-474.5 209,-474.5 209,-438.5 47,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-452.8\">Encoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140124662070352 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140124662070352</title>\n",
       "<polygon fill=\"none\" points=\"18,-365.5 18,-401.5 238,-401.5 238,-365.5 18,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-379.8\">Body-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 140124662069736&#45;&gt;140124662070352 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140124662069736-&gt;140124662070352</title>\n",
       "<path d=\"M128,-438.4551C128,-430.3828 128,-420.6764 128,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.5001,-411.5903 128,-401.5904 124.5001,-411.5904 131.5001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140124582006176 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140124582006176</title>\n",
       "<polygon fill=\"none\" points=\"320.5,-365.5 320.5,-401.5 483.5,-401.5 483.5,-365.5 320.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402\" y=\"-379.8\">Decoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140124582006232 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140124582006232</title>\n",
       "<polygon fill=\"none\" points=\"284,-292.5 284,-328.5 520,-328.5 520,-292.5 284,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402\" y=\"-306.8\">Decoder-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 140124582006176&#45;&gt;140124582006232 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140124582006176-&gt;140124582006232</title>\n",
       "<path d=\"M402,-365.4551C402,-357.3828 402,-347.6764 402,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"405.5001,-338.5903 402,-328.5904 398.5001,-338.5904 405.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140124662070240 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140124662070240</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 256,-328.5 256,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-306.8\">Encoder-Batchnorm-1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140124662070352&#45;&gt;140124662070240 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140124662070352-&gt;140124662070240</title>\n",
       "<path d=\"M128,-365.4551C128,-357.3828 128,-347.6764 128,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.5001,-338.5903 128,-328.5904 124.5001,-338.5904 131.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140124582006288 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140124582006288</title>\n",
       "<polygon fill=\"none\" points=\"273.5,-219.5 273.5,-255.5 530.5,-255.5 530.5,-219.5 273.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"402\" y=\"-233.8\">Decoder-Batchnorm-1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140124582006232&#45;&gt;140124582006288 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140124582006232-&gt;140124582006288</title>\n",
       "<path d=\"M402,-292.4551C402,-284.3828 402,-274.6764 402,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"405.5001,-265.5903 402,-255.5904 398.5001,-265.5904 405.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140127001475728 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140127001475728</title>\n",
       "<polygon fill=\"none\" points=\"1,-219.5 1,-255.5 255,-255.5 255,-219.5 1,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128\" y=\"-233.8\">bidirectional_1(gru_1): Bidirectional(GRU)</text>\n",
       "</g>\n",
       "<!-- 140124662070240&#45;&gt;140127001475728 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140124662070240-&gt;140127001475728</title>\n",
       "<path d=\"M128,-292.4551C128,-284.3828 128,-274.6764 128,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"131.5001,-265.5903 128,-255.5904 124.5001,-265.5904 131.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140124582006736 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140124582006736</title>\n",
       "<polygon fill=\"none\" points=\"199.5,-146.5 199.5,-182.5 330.5,-182.5 330.5,-146.5 199.5,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265\" y=\"-160.8\">Decoder-GRU: GRU</text>\n",
       "</g>\n",
       "<!-- 140124582006288&#45;&gt;140124582006736 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140124582006288-&gt;140124582006736</title>\n",
       "<path d=\"M368.1348,-219.4551C349.9393,-209.7596 327.3193,-197.7066 307.9036,-187.361\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"309.4216,-184.2041 298.9504,-182.5904 306.1298,-190.3818 309.4216,-184.2041\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140127001475728&#45;&gt;140124582006736 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140127001475728-&gt;140124582006736</title>\n",
       "<path d=\"M161.8652,-219.4551C180.0607,-209.7596 202.6807,-197.7066 222.0964,-187.361\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"223.8702,-190.3818 231.0496,-182.5904 220.5784,-184.2041 223.8702,-190.3818\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140124581609880 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140124581609880</title>\n",
       "<polygon fill=\"none\" points=\"136.5,-73.5 136.5,-109.5 393.5,-109.5 393.5,-73.5 136.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265\" y=\"-87.8\">Decoder-Batchnorm-2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140124582006736&#45;&gt;140124581609880 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140124582006736-&gt;140124581609880</title>\n",
       "<path d=\"M265,-146.4551C265,-138.3828 265,-128.6764 265,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"268.5001,-119.5903 265,-109.5904 261.5001,-119.5904 268.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140124581493336 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140124581493336</title>\n",
       "<polygon fill=\"none\" points=\"182,-.5 182,-36.5 348,-36.5 348,-.5 182,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"265\" y=\"-14.8\">Final-Output-Dense: Dense</text>\n",
       "</g>\n",
       "<!-- 140124581609880&#45;&gt;140124581493336 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140124581609880-&gt;140124581493336</title>\n",
       "<path d=\"M265,-73.4551C265,-65.3828 265,-55.6764 265,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"268.5001,-46.5903 265,-36.5904 261.5001,-46.5904 268.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import SVG, display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "display(SVG(model_to_dot(seq2seq_Model).create(prog='dot', format='svg')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a look at all the layers and their shapes and number of training parameters by using the summary function in keras. This function is incredibly useful in making sure all your layers are working the way they should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Encoder-Input (InputLayer)      (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Body-Word-Embedding (Embedding) (None, 13, 20)       280         Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 20)     280         Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Batchnorm-1 (BatchNorma (None, 13, 20)       80          Body-Word-Embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 20)     80          Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512)          1637376     Encoder-Batchnorm-1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               (None, None, 512)    818688      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 512)    2048        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14)     7182        Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 2,466,014\n",
      "Trainable params: 2,464,910\n",
      "Non-trainable params: 1,104\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Inputs and Labels for the Network for Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one last step to go before we can start training. As I said earlier, in teacher forcing we feed in a particular token of the output sequence to the decoder and try to make it predict the next token in the sequence. In order for this to happen we define the decoder inputs as the 1st to N-1th tokens in the output sequence, while the decoder labels it tries to predict are the 2nd to the Nth element of the series. We create these arrays for both the train and test dataset. Note that no transformation needs to be made for the encoder data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dec_inp =train_Y_padded[:, :-1] # Training Decoder Input\n",
    "train_dec_out = train_Y_padded[:, 1:]  # Training Decoder Labels\n",
    "train_enc_inp=train_X_padded ## Training Encoder Input\n",
    "test_dec_inp =test_Y_padded[:, :-1] # Testing Decoder Input\n",
    "test_dec_out = test_Y_padded[:, 1:] # Testing Decoder Labels\n",
    "test_enc_inp=test_X_padded # Testing Encoder Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the result of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Sequence: [ 6 11  7  1  9  7 12 10  2  0  0  0  0]\n",
      "Encoder Input converted back to characters ['4', '3', '9', '-', '6', '9', '0', '8', '$', '#', '#', '#', '#']\n",
      "Decoder Input Sequence [ 1  4  9  7  9 12]\n",
      "Decoder Input converted back to characters ['^', '-', '6', '4', '6', '9']\n",
      "Decoder Output Sequence [ 4  9  7  9 12  2]\n",
      "Decoder Output converted back to characters ['-', '6', '4', '6', '9', '$']\n"
     ]
    }
   ],
   "source": [
    "id=0\n",
    "print(\"Encoder Input Sequence:\",train_enc_inp[id])\n",
    "print(\"Encoder Input converted back to characters\",convert_to_chars(train_enc_inp[id],input_id2char))\n",
    "print(\"Decoder Input Sequence\",train_dec_inp[id])\n",
    "print(\"Decoder Input converted back to characters\",convert_to_chars(train_dec_inp[id],output_id2char))\n",
    "print(\"Decoder Output Sequence\",train_dec_out[id])\n",
    "print(\"Decoder Output converted back to characters\",convert_to_chars(train_dec_out[id],output_id2char))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start training the model. The Model.fit() command does the training. We have to pass the model the training and testing (validation) data. We also specify the number of epochs and the batch size. Finally we use the Keras ModelChekpoint callback to save the best weights during our training. The best model is decided on the basis of the validation loss.\n",
    "\n",
    "Note-  The BatchNormalization Layer behaves slightly differently between training and testing. So we set the Keras Learning Phase to 1 to show that right now we are in training. 0 would indicate testing/ Inference. Note that that while outputting validation losses, Keras automatically takes care of setting the learning phase. We set it here mainly because we will be resetting it during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/20\n",
      "800000/800000 [==============================] - 49s 61us/step - loss: 1.9769 - val_loss: 1.4077\n",
      "Epoch 2/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 1.3005 - val_loss: 1.2218\n",
      "Epoch 3/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 1.1751 - val_loss: 1.2605\n",
      "Epoch 4/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 1.1162 - val_loss: 1.0575\n",
      "Epoch 5/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 1.0576 - val_loss: 0.9659\n",
      "Epoch 6/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.8285 - val_loss: 0.7413\n",
      "Epoch 7/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.7229 - val_loss: 0.6814\n",
      "Epoch 8/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.6439 - val_loss: 0.6795\n",
      "Epoch 9/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.5748 - val_loss: 0.5162\n",
      "Epoch 10/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.4500 - val_loss: 0.3595\n",
      "Epoch 11/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.3229 - val_loss: 0.3011\n",
      "Epoch 12/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.2452 - val_loss: 0.1941\n",
      "Epoch 13/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.1662 - val_loss: 0.0734\n",
      "Epoch 14/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0375 - val_loss: 0.0224\n",
      "Epoch 15/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0153 - val_loss: 0.0148\n",
      "Epoch 16/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0114 - val_loss: 0.0153\n",
      "Epoch 17/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0091 - val_loss: 0.0091\n",
      "Epoch 18/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0070 - val_loss: 0.0100\n",
      "Epoch 19/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0211 - val_loss: 0.0073\n",
      "Epoch 20/20\n",
      "800000/800000 [==============================] - 48s 60us/step - loss: 0.0043 - val_loss: 0.0048\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "K.set_learning_phase(1)\n",
    "model_checkpoint = ModelCheckpoint('best_weights.hdf5',\n",
    "                                   save_best_only=True)\n",
    "batch_size = 5120\n",
    "epochs = 20\n",
    "history = seq2seq_Model.fit(x=[train_enc_inp, train_dec_inp],y=np.expand_dims(train_dec_out,-1),epochs=epochs,\n",
    "                            validation_data=[[test_enc_inp,test_dec_inp],np.expand_dims(test_dec_out,-1)],batch_size=batch_size,\n",
    "                            callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Using the trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the best weights during our training and set the learning phase to 0 for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq_Model.load_weights(\"best_weights.hdf5\")\n",
    "seq2seq_Model.load_weights(\"saved_awesome_weights.hdf5\")\n",
    "K.set_learning_phase(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder for Inference\n",
    "\n",
    "Getting the encoder back from the trained model is simple. We just define a new model using the first and last layer of our encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encoder(model):\n",
    "    return Model(model.get_layer(\"Encoder-Input\").input,model.get_layer(\"bidirectional_1\").output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder for Inference\n",
    "\n",
    "Recoonstructing the decoder for Inference involves recalling the layers one by one (There is a nicer way to do this by encapsulating the models earlier but this works for now). However we add an extra input layer where we take in the output of the encoder model and pass it as the initial state of the Decoder RNN. We also get not only the output of the Dense projection layer but also the RNN output state as our outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_decoder(model):\n",
    "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
    "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
    "    gru_inference_state_input = Input(shape=(hidden_size,), name='hidden_state_input')\n",
    "    gru_out = model.get_layer('Decoder-GRU')(dec_bn, initial_state=gru_inference_state_input)\n",
    "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
    "    decoder_model = Model([decoder_inputs, gru_inference_state_input],\n",
    "                          [dense_out, gru_out])\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Loop\n",
    "\n",
    "To actually run inference, we create the function below. The function first extracts the encoder and decoder from the seq2seq model. Then we pass the input string to the encoder and get the encoded output. We then feed this encoder output as the initial state input for the decoder model. For the sequence input character, we seed the sequence with the start character. We then run the decoder one step to get the first predicted output character and the output state of the RNN.  We then take the output character as our new input character and the RNN state as our new RNN state and run the decoder in a loop till we either get the Eend symbol as our output or we hit a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_on_sample(input_array,model=seq2seq_Model):\n",
    "    encoder_model=extract_encoder(model)\n",
    "    decoder_model=extract_decoder(model)\n",
    "    body_encoding = encoder_model.predict(input_array)\n",
    "    state_value = np.array(output_char2id[start_symbol]).reshape(1, 1)\n",
    "    decoded_seq = []\n",
    "    while True:\n",
    "        preds, st = decoder_model.predict([state_value, body_encoding])\n",
    "        sampled_token_index = np.argmax(preds)\n",
    "        sampled_char = output_id2char[sampled_token_index]\n",
    "        if sampled_char ==end_symbol or len(decoded_seq) >output_max_len:\n",
    "                break\n",
    "        decoded_seq.append(sampled_char)\n",
    "        body_encoding = st[0]\n",
    "        state_value = np.array(sampled_token_index).reshape(1, 1)\n",
    "\n",
    "    return ''.join(decoded_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains a couple of functions to calculate and pretty print the inference results on some elements of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_on_array(inp_array,actual_outputs):\n",
    "    for i,j in enumerate(inp_array):\n",
    "        print(f\"========= Example {i} ============\")\n",
    "        inp_string=\"\".join(convert_to_chars(j,input_id2char))\n",
    "        label_string=\"\".join(convert_to_chars(actual_outputs[i],output_id2char))\n",
    "        print(f\"Input Expression --> {inp_string.split(end_symbol)[0]}\")\n",
    "        print(f\"Actual Output Expression --> {label_string.lstrip(start_symbol).split(end_symbol)[0]}\")\n",
    "        pred= infer_on_sample(np.expand_dims(j,0))\n",
    "        print(f\"Predicted Output --> {pred}\")\n",
    "        print(\"====================================\")\n",
    "    return\n",
    "\n",
    "def gen_random_inferences(n=20):\n",
    "    sz=test_X_padded.shape[0]\n",
    "    random_indices=np.random.randint(0,sz,size=n)\n",
    "    return infer_on_array(test_X_padded[random_indices],test_Y_padded[random_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how we did. If your training went well, you should see an almost perfect calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Example 0 ============\n",
      "Input Expression --> -1023+-6098\n",
      "Actual Output Expression --> -7121\n",
      "Predicted Output --> -7121\n",
      "====================================\n",
      "========= Example 1 ============\n",
      "Input Expression --> 4942--7686\n",
      "Actual Output Expression --> 12628\n",
      "Predicted Output --> 12628\n",
      "====================================\n",
      "========= Example 2 ============\n",
      "Input Expression --> 3303+-9481\n",
      "Actual Output Expression --> -6178\n",
      "Predicted Output --> -6178\n",
      "====================================\n",
      "========= Example 3 ============\n",
      "Input Expression --> -5353--5996\n",
      "Actual Output Expression --> 643\n",
      "Predicted Output --> 643\n",
      "====================================\n",
      "========= Example 4 ============\n",
      "Input Expression --> -6958--4340\n",
      "Actual Output Expression --> -2618\n",
      "Predicted Output --> -2618\n",
      "====================================\n",
      "========= Example 5 ============\n",
      "Input Expression --> 1892--5676\n",
      "Actual Output Expression --> 7568\n",
      "Predicted Output --> 7568\n",
      "====================================\n",
      "========= Example 6 ============\n",
      "Input Expression --> 9549+8526\n",
      "Actual Output Expression --> 18075\n",
      "Predicted Output --> 18075\n",
      "====================================\n",
      "========= Example 7 ============\n",
      "Input Expression --> 1668+7104\n",
      "Actual Output Expression --> 8772\n",
      "Predicted Output --> 8772\n",
      "====================================\n",
      "========= Example 8 ============\n",
      "Input Expression --> -8864+-1678\n",
      "Actual Output Expression --> -10542\n",
      "Predicted Output --> -10542\n",
      "====================================\n",
      "========= Example 9 ============\n",
      "Input Expression --> 3132-6493\n",
      "Actual Output Expression --> -3361\n",
      "Predicted Output --> -3361\n",
      "====================================\n",
      "========= Example 10 ============\n",
      "Input Expression --> 2204--7910\n",
      "Actual Output Expression --> 10114\n",
      "Predicted Output --> 10114\n",
      "====================================\n",
      "========= Example 11 ============\n",
      "Input Expression --> 6750+-8069\n",
      "Actual Output Expression --> -1319\n",
      "Predicted Output --> -1319\n",
      "====================================\n",
      "========= Example 12 ============\n",
      "Input Expression --> 7598+-6876\n",
      "Actual Output Expression --> 722\n",
      "Predicted Output --> 722\n",
      "====================================\n",
      "========= Example 13 ============\n",
      "Input Expression --> 9727-2301\n",
      "Actual Output Expression --> 7426\n",
      "Predicted Output --> 7426\n",
      "====================================\n",
      "========= Example 14 ============\n",
      "Input Expression --> -3936--6747\n",
      "Actual Output Expression --> 2811\n",
      "Predicted Output --> 2811\n",
      "====================================\n",
      "========= Example 15 ============\n",
      "Input Expression --> 8460--3602\n",
      "Actual Output Expression --> 12062\n",
      "Predicted Output --> 12062\n",
      "====================================\n",
      "========= Example 16 ============\n",
      "Input Expression --> 6896-1969\n",
      "Actual Output Expression --> 4927\n",
      "Predicted Output --> 4927\n",
      "====================================\n",
      "========= Example 17 ============\n",
      "Input Expression --> -2625--9504\n",
      "Actual Output Expression --> 6879\n",
      "Predicted Output --> 6879\n",
      "====================================\n",
      "========= Example 18 ============\n",
      "Input Expression --> -728+1097\n",
      "Actual Output Expression --> 369\n",
      "Predicted Output --> 369\n",
      "====================================\n",
      "========= Example 19 ============\n",
      "Input Expression --> -6360+-6838\n",
      "Actual Output Expression --> -13198\n",
      "Predicted Output --> -13198\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "gen_random_inferences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all folks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
